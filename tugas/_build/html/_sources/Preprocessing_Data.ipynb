{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1c1551",
   "metadata": {},
   "source": [
    "## PrepocessingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "589ec587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPROCESSING NOTEBOOK\n",
      "==================================================\n",
      "Tahap preprocessing data untuk dataset Iris\n",
      "Berdasarkan hasil analisis dari DataUnderstanding notebook\n",
      "Penanganan outliers dari multi-model PyCaret (ABOD, KNN, COF)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Notebook ini berisi tahap preprocessing data untuk dataset Iris berdasarkan hasil analisis dari DataUnderstanding notebook, termasuk penanganan outliers yang terdeteksi oleh multi-model PyCaret (ABOD, KNN, COF).\n",
    "\n",
    "print(\"DATA PREPROCESSING NOTEBOOK\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Tahap preprocessing data untuk dataset Iris\")\n",
    "print(\"Berdasarkan hasil analisis dari DataUnderstanding notebook\")\n",
    "print(\"Penanganan outliers dari multi-model PyCaret (ABOD, KNN, COF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68047d2e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries dan Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3997e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyCaret tidak tersedia atau tidak kompatibel dengan versi Python Anda.\n",
      "Pesan error: ('Pycaret only supports python 3.9, 3.10, 3.11. Your actual Python version: ', sys.version_info(major=3, minor=12, micro=1, releaselevel='final', serial=0), 'Please DOWNGRADE your Python version.')\n",
      "PyCaret hanya mendukung Python 3.9, 3.10, 3.11. Silakan gunakan versi Python yang sesuai atau lanjutkan tanpa PyCaret.\n",
      "Libraries berhasil diimport untuk preprocessing\n",
      "Libraries yang tersedia:\n",
      "   â€¢ Pandas & NumPy: Data manipulation\n",
      "   â€¢ Matplotlib & Seaborn: Visualisasi\n",
      "   â€¢ Scikit-learn: Preprocessing tools\n",
      "   â€¢ PyCaret: Advanced ML preprocessing\n",
      "\n",
      "=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\n",
      "Error loading data: 'NoneType' object has no attribute 'rename'\n"
     ]
    }
   ],
   "source": [
    "# Import libraries yang diperlukan untuk preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import PyCaret untuk preprocessing dan modeling\n",
    "try:\n",
    "    from pycaret.datasets import get_data\n",
    "    from pycaret.classification import *\n",
    "    from pycaret.anomaly import *\n",
    "    print(\"PyCaret berhasil diimport\")\n",
    "except (ImportError, RuntimeError) as e:\n",
    "    print(\"PyCaret tidak tersedia atau tidak kompatibel dengan versi Python Anda.\")\n",
    "    print(\"Pesan error:\", e)\n",
    "    print(\"PyCaret hanya mendukung Python 3.9, 3.10, 3.11. Silakan gunakan versi Python yang sesuai atau lanjutkan tanpa PyCaret.\")\n",
    "\n",
    "# Atur style untuk visualisasi\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries berhasil diimport untuk preprocessing\")\n",
    "print(\"Libraries yang tersedia:\")\n",
    "print(\"   â€¢ Pandas & NumPy: Data manipulation\")\n",
    "print(\"   â€¢ Matplotlib & Seaborn: Visualisasi\")\n",
    "print(\"   â€¢ Scikit-learn: Preprocessing tools\")\n",
    "print(\"   â€¢ PyCaret: Advanced ML preprocessing\")\n",
    "\n",
    "# === LOAD DATASET IRIS ===\n",
    "print(\"\\n=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\")\n",
    "\n",
    "try:\n",
    "    # Load dataset IRIS.csv (pakai koma sebagai separator)\n",
    "    IRIS = pd.read_csv(\"IRIS.csv\")\n",
    "\n",
    "\n",
    "    # Rename kolom supaya konsisten dengan sklearn\n",
    "    df = df.rename(columns={\n",
    "        'sepal_length': 'sepal length (cm)',\n",
    "        'sepal_width': 'sepal width (cm)',\n",
    "        'petal_length': 'petal length (cm)',\n",
    "        'petal_width': 'petal width (cm)',\n",
    "        'species': 'Class'\n",
    "    })\n",
    "\n",
    "    # Tambah kolom species numerik & species name\n",
    "    df['species'] = df['Class'].map({\n",
    "        'Iris-setosa': 0,\n",
    "        'Iris-versicolor': 1,\n",
    "        'Iris-virginica': 2\n",
    "    })\n",
    "    df['species_name'] = df['Class'].map({\n",
    "        'Iris-setosa': 'setosa',\n",
    "        'Iris-versicolor': 'versicolor',\n",
    "        'Iris-virginica': 'virginica'\n",
    "    })\n",
    "\n",
    "    # Hapus kolom 'Class' biar dataset bersih\n",
    "    df = df.drop('Class', axis=1)\n",
    "\n",
    "    # Info dataset\n",
    "    features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "    print(f\"\\nInfo Dataset:\")\n",
    "    print(f\"   â€¢ Ukuran: {df.shape[0]} baris, {df.shape[1]} kolom\")\n",
    "    print(f\"   â€¢ Features: {features}\")\n",
    "    print(f\"   â€¢ Target: species (0=setosa, 1=versicolor, 2=virginica)\")\n",
    "\n",
    "    print(\"\\nSample Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    df = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b3e04",
   "metadata": {},
   "source": [
    "## 2. Load Data dan Hasil Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6660f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\n",
      "Error loading data: 'Class'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset Iris dengan hasil outlier detection dari DataUnderstanding\n",
    "print(\"=== LOADING DATA DAN HASIL OUTLIER DETECTION ===\")\n",
    "\n",
    "try:\n",
    "    # Load data dari file CSV atau PyCaret\n",
    "    try:\n",
    "        # Coba load dari file lokal\n",
    "        df = pd.read_csv('IRIS.csv', delimiter=';')\n",
    "        \n",
    "        # Konversi kolom numerik yang menggunakan koma sebagai decimal separator\n",
    "        numeric_columns = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in df.columns:\n",
    "                # Konversi koma ke titik untuk decimal\n",
    "                df[col] = df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "        \n",
    "        # Buat kolom species numerik dan species name\n",
    "        df['species'] = df['Class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "        df['species_name'] = df['Class'].map({'Iris-setosa': 'setosa', 'Iris-versicolor': 'versicolor', 'Iris-virginica': 'virginica'})\n",
    "        \n",
    "        # Rename kolom untuk konsistensi dengan format sklearn\n",
    "        df = df.rename(columns={\n",
    "            'sepal length': 'sepal length (cm)',\n",
    "            'sepal width': 'sepal width (cm)', \n",
    "            'petal length': 'petal length (cm)',\n",
    "            'petal width': 'petal width (cm)'\n",
    "        })\n",
    "        \n",
    "        # Drop kolom yang tidak diperlukan\n",
    "        if 'id' in df.columns:\n",
    "            df = df.drop('id', axis=1)\n",
    "        if 'Class' in df.columns:\n",
    "            df = df.drop('Class', axis=1)\n",
    "        \n",
    "        print(\"Dataset Iris berhasil dimuat dari data_iris.csv\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # Jika file tidak ditemukan, gunakan dataset Iris dari PyCaret\n",
    "        print(\"File lokal tidak ditemukan. Menggunakan dataset Iris dari PyCaret...\")\n",
    "        try:\n",
    "            df = get_data('iris')\n",
    "            df['species'] = df['species'].map({'setosa': 0, 'versicolor': 1, 'virginica': 2})\n",
    "            df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "            print(\"Dataset Iris berhasil dimuat dari PyCaret\")\n",
    "        except:\n",
    "            print(\"Error: Tidak dapat memuat dataset dari PyCaret\")\n",
    "    \n",
    "    # Define feature columns\n",
    "    features = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "    \n",
    "    print(f\"\\nInfo Dataset:\")\n",
    "    print(f\"   â€¢ Ukuran: {df.shape[0]} baris, {df.shape[1]} kolom\")\n",
    "    print(f\"   â€¢ Features: {features}\")\n",
    "    print(f\"   â€¢ Target: species (0=setosa, 1=versicolor, 2=virginica)\")\n",
    "    \n",
    "    # Tampilkan sample data\n",
    "    print(f\"\\nSample Data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7690d42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETEKSI OUTLIER DENGAN PYCARET ===\n",
      "Data tidak tersedia\n"
     ]
    }
   ],
   "source": [
    "# Outlier Detection menggunakan PyCaret\n",
    "print(\"=== DETEKSI OUTLIER DENGAN PYCARET ===\")\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Menggunakan 3 model: ABOD, KNN, COF\")\n",
    "    \n",
    "    try:\n",
    "        # Setup PyCaret anomaly detection\n",
    "        from pycaret.anomaly import setup as anomaly_setup, create_model, assign_model\n",
    "        \n",
    "        anomaly_env = anomaly_setup(\n",
    "            data=df[features],\n",
    "            session_id=42,\n",
    "            train_size=1.0,\n",
    "            silent=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ PyCaret setup berhasil\")\n",
    "        \n",
    "        # Jalankan 3 model outlier detection\n",
    "        outlier_results = {}\n",
    "        models = ['abod', 'knn', 'cof']\n",
    "        \n",
    "        for model_name in models:\n",
    "            try:\n",
    "                model = create_model(model_name, fraction=0.1)\n",
    "                results = assign_model(model)\n",
    "                outliers = results[results['Anomaly'] == 1].index.tolist()\n",
    "                outlier_results[model_name] = outliers\n",
    "                df[f'{model_name}_outlier'] = results['Anomaly']\n",
    "                \n",
    "                print(f\"âœ“ {model_name.upper()}: {len(outliers)} outliers\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— {model_name.upper()}: error\")\n",
    "                outlier_results[model_name] = []\n",
    "                df[f'{model_name}_outlier'] = 0\n",
    "        \n",
    "        # Consensus analysis\n",
    "        df['consensus_score'] = df['abod_outlier'] + df['knn_outlier'] + df['cof_outlier']\n",
    "        df['strong_consensus'] = (df['consensus_score'] >= 2).astype(int)\n",
    "        \n",
    "        strong_count = df['strong_consensus'].sum()\n",
    "        print(f\"\\nðŸ“Š Consensus outliers (â‰¥2 model): {strong_count}\")\n",
    "        \n",
    "        if strong_count > 0:\n",
    "            strong_indices = df[df['strong_consensus'] == 1].index.tolist()\n",
    "            print(f\"   Indices: {strong_indices}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error PyCaret: {e}\")\n",
    "        print(\"Menggunakan metode statistik...\")\n",
    "        \n",
    "        # Fallback sederhana\n",
    "        outlier_results = {'abod': [], 'knn': [], 'cof': []}\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            Q1 = df[feature].quantile(0.25)\n",
    "            Q3 = df[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = df[(df[feature] < Q1 - 1.5*IQR) | (df[feature] > Q3 + 1.5*IQR)].index.tolist()\n",
    "            \n",
    "            model_name = ['abod', 'knn', 'cof'][i % 3]\n",
    "            outlier_results[model_name].extend(outliers[:3])\n",
    "        \n",
    "        for model_name, outliers in outlier_results.items():\n",
    "            df[f'{model_name}_outlier'] = 0\n",
    "            if outliers:\n",
    "                df.loc[outliers, f'{model_name}_outlier'] = 1\n",
    "        \n",
    "        df['consensus_score'] = df['abod_outlier'] + df['knn_outlier'] + df['cof_outlier']\n",
    "        df['strong_consensus'] = (df['consensus_score'] >= 2).astype(int)\n",
    "        \n",
    "        print(\"âœ“ Statistik outlier detection selesai\")\n",
    "        \n",
    "else:\n",
    "    print(\"Data tidak tersedia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b0917",
   "metadata": {},
   "source": [
    "## 3. Outlier Removal dan Imputation dengan Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2c9b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER REMOVAL DAN IMPUTATION PROCESS ===\n",
      "Tidak dapat melakukan outlier removal karena data atau hasil outlier tidak tersedia\n"
     ]
    }
   ],
   "source": [
    "# Outlier Removal dan Imputation Process\n",
    "print(\"=== OUTLIER REMOVAL DAN IMPUTATION PROCESS ===\")\n",
    "\n",
    "if df is not None and 'consensus_score' in df.columns:\n",
    "    \n",
    "    # Identifikasi outliers berdasarkan strong consensus (â‰¥2 models agree)\n",
    "    outlier_mask = df['strong_consensus'] == 1\n",
    "    outlier_indices = df[outlier_mask].index.tolist()\n",
    "    outlier_count = len(outlier_indices)\n",
    "    \n",
    "    print(f\"\\nOutliers teridentifikasi (Strong Consensus):\")\n",
    "    print(f\"   â€¢ Jumlah: {outlier_count} dari {len(df)} data ({outlier_count/len(df)*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Indices: {outlier_indices}\")\n",
    "    \n",
    "    # STEP 1: Visualisasi Koordinat Data Original\n",
    "    print(f\"\\nSTEP 1: Visualisasi Koordinat Data Original\")\n",
    "    \n",
    "    # Tampilkan koordinat outliers dan normal data\n",
    "    print(f\"\\nKoordinat Outliers (Strong Consensus):\")\n",
    "    print(f\"{'Index':<6} {'Sepal_L':<8} {'Sepal_W':<8} {'Petal_L':<8} {'Petal_W':<8} {'Species':<8}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    outlier_coords = df[outlier_mask]\n",
    "    for idx in outlier_indices:\n",
    "        row = df.loc[idx]\n",
    "        species_name = ['setosa', 'versicolor', 'virginica'][int(row['species'])]\n",
    "        print(f\"{idx:<6} {row[features[0]]:<8.2f} {row[features[1]]:<8.2f} {row[features[2]]:<8.2f} {row[features[3]]:<8.2f} {species_name:<8}\")\n",
    "    \n",
    "    # Visualisasi scatter plot koordinat 2D\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Koordinat Data Original - Normal vs Outliers', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Sepal Length vs Sepal Width\n",
    "    normal_data = df[~outlier_mask]\n",
    "    outlier_data = df[outlier_mask]\n",
    "    \n",
    "    # Normal data\n",
    "    axes[0].scatter(normal_data[features[0]], normal_data[features[1]], \n",
    "                   c=normal_data['species'], cmap='viridis', alpha=0.7, s=50, label='Normal')\n",
    "    \n",
    "    # Outlier data\n",
    "    if len(outlier_data) > 0:\n",
    "        axes[0].scatter(outlier_data[features[0]], outlier_data[features[1]], \n",
    "                       color='red', s=100, marker='x', label=f'Outliers ({len(outlier_data)})')\n",
    "    \n",
    "    axes[0].set_xlabel('Sepal Length (cm)')\n",
    "    axes[0].set_ylabel('Sepal Width (cm)')\n",
    "    axes[0].set_title('Sepal Coordinates')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Petal Length vs Petal Width\n",
    "    axes[1].scatter(normal_data[features[2]], normal_data[features[3]], \n",
    "                   c=normal_data['species'], cmap='viridis', alpha=0.7, s=50, label='Normal')\n",
    "    \n",
    "    if len(outlier_data) > 0:\n",
    "        axes[1].scatter(outlier_data[features[2]], outlier_data[features[3]], \n",
    "                       color='red', s=100, marker='x', label=f'Outliers ({len(outlier_data)})')\n",
    "    \n",
    "    axes[1].set_xlabel('Petal Length (cm)')\n",
    "    axes[1].set_ylabel('Petal Width (cm)')\n",
    "    axes[1].set_title('Petal Coordinates')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # STEP 2: Remove Outliers (Set as NaN)\n",
    "    print(f\"\\nSTEP 2: Menghapus Outliers (Set sebagai NaN)\")\n",
    "    \n",
    "    # Buat copy dataset dan set outliers sebagai NaN\n",
    "    df_with_nan = df.copy()\n",
    "    \n",
    "    for feature in features:\n",
    "        df_with_nan.loc[outlier_mask, feature] = np.nan\n",
    "    \n",
    "    # Cek jumlah missing values\n",
    "    missing_count = df_with_nan[features].isnull().sum()\n",
    "    print(f\"\\nMissing values setelah outlier removal:\")\n",
    "    for feature in features:\n",
    "        missing = missing_count[feature]\n",
    "        pct = (missing / len(df_with_nan)) * 100\n",
    "        print(f\"   â€¢ {feature}: {missing} missing ({pct:.1f}%)\")\n",
    "    \n",
    "    # STEP 3: Koordinat Data dengan Missing Values\n",
    "    print(f\"\\nSTEP 3: Koordinat Data dengan Missing Values\")\n",
    "    \n",
    "    print(f\"\\nKoordinat Data Setelah Outlier Removal (NaN untuk outliers):\")\n",
    "    print(f\"{'Index':<6} {'Sepal_L':<8} {'Sepal_W':<8} {'Petal_L':<8} {'Petal_W':<8} {'Status':<8}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Show sample missing data coordinates\n",
    "    missing_indices = df_with_nan[df_with_nan[features].isnull().any(axis=1)].index.tolist()\n",
    "    for idx in missing_indices[:5]:  # Show first 5 missing\n",
    "        row = df_with_nan.loc[idx]\n",
    "        print(f\"{idx:<6} {'NaN':<8} {'NaN':<8} {'NaN':<8} {'NaN':<8} {'Missing':<8}\")\n",
    "    \n",
    "    # Visualisasi koordinat data yang tersisa\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Koordinat Data Setelah Outlier Removal', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Data yang masih tersedia (non-NaN)\n",
    "    available_mask = ~df_with_nan[features].isnull().any(axis=1)\n",
    "    available_data = df_with_nan[available_mask]\n",
    "    \n",
    "    # Plot 1: Sepal coordinates\n",
    "    axes[0].scatter(available_data[features[0]], available_data[features[1]], \n",
    "                   c=available_data['species'], cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[0].set_xlabel('Sepal Length (cm)')\n",
    "    axes[0].set_ylabel('Sepal Width (cm)')\n",
    "    axes[0].set_title(f'Sepal Coordinates\\n({len(available_data)} available points)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Petal coordinates\n",
    "    axes[1].scatter(available_data[features[2]], available_data[features[3]], \n",
    "                   c=available_data['species'], cmap='viridis', alpha=0.7, s=50)\n",
    "    axes[1].set_xlabel('Petal Length (cm)')\n",
    "    axes[1].set_ylabel('Petal Width (cm)')\n",
    "    axes[1].set_title(f'Petal Coordinates\\n({len(available_data)} available points)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # STEP 4: Imputation Methods\n",
    "    print(f\"\\nSTEP 4: Metode Imputation untuk Mengisi Missing Values\")\n",
    "    \n",
    "    # Siapkan berbagai metode imputation\n",
    "    imputation_methods = {}\n",
    "    \n",
    "    # Method 1: Mean Imputation\n",
    "    df_mean_imputed = df_with_nan.copy()\n",
    "    for feature in features:\n",
    "        mean_value = df_mean_imputed[feature].mean()\n",
    "        df_mean_imputed[feature].fillna(mean_value, inplace=True)\n",
    "    imputation_methods['mean'] = df_mean_imputed\n",
    "    \n",
    "    # Method 2: Median Imputation\n",
    "    df_median_imputed = df_with_nan.copy()\n",
    "    for feature in features:\n",
    "        median_value = df_median_imputed[feature].median()\n",
    "        df_median_imputed[feature].fillna(median_value, inplace=True)\n",
    "    imputation_methods['median'] = df_median_imputed\n",
    "    \n",
    "    # Method 3: Mode/Most Frequent per Species\n",
    "    df_species_imputed = df_with_nan.copy()\n",
    "    for feature in features:\n",
    "        for species in [0, 1, 2]:\n",
    "            species_mask = df_species_imputed['species'] == species\n",
    "            species_data = df_species_imputed.loc[species_mask, feature]\n",
    "            species_median = species_data.median()\n",
    "            \n",
    "            missing_mask = df_species_imputed[feature].isnull() & species_mask\n",
    "            df_species_imputed.loc[missing_mask, feature] = species_median\n",
    "    imputation_methods['species_median'] = df_species_imputed\n",
    "    \n",
    "    # Method 4: KNN Imputation (Simple version)\n",
    "    from sklearn.impute import KNNImputer\n",
    "    df_knn_imputed = df_with_nan.copy()\n",
    "    \n",
    "    # Hanya impute untuk features numerik\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    imputed_values = knn_imputer.fit_transform(df_knn_imputed[features])\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        df_knn_imputed[feature] = imputed_values[:, i]\n",
    "    imputation_methods['knn'] = df_knn_imputed\n",
    "    \n",
    "    print(f\"\\nMetode Imputation yang digunakan:\")\n",
    "    print(f\"   1. Mean Imputation: Menggunakan rata-rata kolom\")\n",
    "    print(f\"   2. Median Imputation: Menggunakan median kolom\")\n",
    "    print(f\"   3. Species Median: Menggunakan median per spesies\")\n",
    "    print(f\"   4. KNN Imputation: Menggunakan 5 nearest neighbors\")\n",
    "    \n",
    "    # STEP 5: Koordinat Perbandingan Metode Imputation\n",
    "    print(f\"\\nSTEP 5: Koordinat Perbandingan Metode Imputation\")\n",
    "    \n",
    "    # Show imputed coordinates untuk outlier indices\n",
    "    print(f\"\\nKoordinat Imputed untuk Outlier Indices:\")\n",
    "    print(f\"{'Method':<15} {'Index':<6} {'Sepal_L':<8} {'Sepal_W':<8} {'Petal_L':<8} {'Petal_W':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Show coordinates for first few outlier indices across methods\n",
    "    sample_outlier_idx = outlier_indices[0] if outlier_indices else 0\n",
    "    \n",
    "    for method_name, df_imputed in list(imputation_methods.items())[:4]:\n",
    "        method_display = {\n",
    "            'mean': 'Mean',\n",
    "            'median': 'Median', \n",
    "            'species_median': 'Species Med',\n",
    "            'knn': 'KNN'\n",
    "        }.get(method_name, method_name)\n",
    "        \n",
    "        row = df_imputed.loc[sample_outlier_idx]\n",
    "        print(f\"{method_display:<15} {sample_outlier_idx:<6} {row[features[0]]:<8.2f} {row[features[1]]:<8.2f} {row[features[2]]:<8.2f} {row[features[3]]:<8.2f}\")\n",
    "    \n",
    "    # Visualisasi perbandingan koordinat\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Koordinat Perbandingan Metode Imputation', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['skyblue', 'lightcoral', 'lightpink', 'lightyellow']\n",
    "    method_names = ['Mean', 'Median', 'Species Median', 'KNN']\n",
    "    \n",
    "    for i, (method, df_imputed) in enumerate(list(imputation_methods.items())[:4]):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Normal data (tidak berubah)\n",
    "        normal_mask = ~df[features].index.isin(outlier_indices)\n",
    "        normal_data = df_imputed[normal_mask]\n",
    "        \n",
    "        # Imputed data (yang tadinya outliers)\n",
    "        imputed_data = df_imputed[df_imputed.index.isin(outlier_indices)]\n",
    "        \n",
    "        # Plot normal data\n",
    "        axes[row, col].scatter(normal_data[features[0]], normal_data[features[1]], \n",
    "                             c=normal_data['species'], cmap='viridis', alpha=0.6, s=30, label='Normal')\n",
    "        \n",
    "        # Plot imputed outliers\n",
    "        if len(imputed_data) > 0:\n",
    "            axes[row, col].scatter(imputed_data[features[0]], imputed_data[features[1]], \n",
    "                                 color='red', s=80, marker='s', alpha=0.8, \n",
    "                                 label=f'Imputed ({len(imputed_data)})')\n",
    "        \n",
    "        axes[row, col].set_xlabel('Sepal Length (cm)')\n",
    "        axes[row, col].set_ylabel('Sepal Width (cm)')\n",
    "        axes[row, col].set_title(f'{method_names[i]} Imputation')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # STEP 6: Evaluasi Metode Imputation\n",
    "    print(f\"\\nSTEP 6: Pilih Metode Imputation Terbaik\")\n",
    "    \n",
    "    # Pilih metode terbaik (species median untuk Iris dataset)\n",
    "    recommended_method = 'species_median'\n",
    "    df_processed = imputation_methods[recommended_method].copy()\n",
    "    \n",
    "    print(f\"\\nâœ“ Metode dipilih: SPECIES MEDIAN\")\n",
    "    print(f\"   Alasan: Sesuai dengan karakteristik biologis per spesies\")\n",
    "    \n",
    "    # Simple comparison table\n",
    "    focus_feature = 'sepal length (cm)'\n",
    "    print(f\"\\nPerbandingan hasil imputation ({focus_feature}):\")\n",
    "    print(f\"{'Method':<12} {'Mean':<8}\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Show simple stats\n",
    "    original_mean = df[focus_feature].mean()\n",
    "    print(f\"{'Original':<12} {original_mean:<7.2f}\")\n",
    "    \n",
    "    for method, df_imputed in list(imputation_methods.items())[:3]:\n",
    "        method_name = {'mean': 'Mean', 'median': 'Median', 'species_median': 'Species'}.get(method, method)\n",
    "        imputed_mean = df_imputed[focus_feature].mean()\n",
    "        print(f\"{method_name:<12} {imputed_mean:<7.2f}\")\n",
    "    \n",
    "    print(f\"\\nâ†’ Species median dipilih karena mempertahankan perbedaan antar spesies\")\n",
    "    \n",
    "    # STEP 7: Hasil Akhir Preprocessing\n",
    "    print(f\"\\nSTEP 7: Hasil Akhir Preprocessing\")\n",
    "    \n",
    "    # Tampilkan summary singkat\n",
    "    print(f\"\\nPreprocessing Summary:\")\n",
    "    print(f\"   â€¢ Outliers ditemukan: {outlier_count} data points\")\n",
    "    print(f\"   â€¢ Metode imputation: Species-based median\") \n",
    "    print(f\"   â€¢ Dataset size tetap: 150 samples\")\n",
    "    \n",
    "    # Koordinat comparison sederhana\n",
    "    print(f\"\\nContoh perubahan koordinat outliers:\")\n",
    "    for idx in outlier_indices[:2]:  # Show first 2\n",
    "        before = df.loc[idx, features[:2]]\n",
    "        after = df_processed.loc[idx, features[:2]]\n",
    "        print(f\"   Index {idx}: {before.values} â†’ {after.values}\")\n",
    "    \n",
    "    # Visualisasi before vs after\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Before (dengan outliers)\n",
    "    for species in df['species'].unique():\n",
    "        species_data = df[df['species'] == species]\n",
    "        ax1.scatter(species_data[features[0]], species_data[features[1]], \n",
    "                   label=species, alpha=0.7)\n",
    "    # Mark outliers\n",
    "    outliers_original = df.loc[outlier_indices]\n",
    "    ax1.scatter(outliers_original[features[0]], outliers_original[features[1]], \n",
    "               color='red', marker='x', s=100, label='Outliers')\n",
    "    ax1.set_title('Before: Data dengan Outliers')\n",
    "    ax1.set_xlabel('Sepal Length (cm)')\n",
    "    ax1.set_ylabel('Sepal Width (cm)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # After (imputed)\n",
    "    for species in df_processed['species'].unique():\n",
    "        species_data = df_processed[df_processed['species'] == species]\n",
    "        ax2.scatter(species_data[features[0]], species_data[features[1]], \n",
    "                   label=species, alpha=0.7)\n",
    "    # Mark imputed points\n",
    "    imputed_points = df_processed.loc[outlier_indices]\n",
    "    ax2.scatter(imputed_points[features[0]], imputed_points[features[1]], \n",
    "               color='green', marker='s', s=100, label='Imputed')\n",
    "    ax2.set_title('After: Data dengan Imputation')\n",
    "    ax2.set_xlabel('Sepal Length (cm)')\n",
    "    ax2.set_ylabel('Sepal Width (cm)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nâœ“ PREPROCESSING OUTLIER SELESAI\")\n",
    "    print(f\"   Data siap untuk feature scaling dan modeling\")\n",
    "    \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan outlier removal karena data atau hasil outlier tidak tersedia\")\n",
    "    df_processed = df.copy() if df is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269c0754",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling dan Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "397ca670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SCALING DAN ENGINEERING DENGAN PYCARET ===\n",
      "Tidak dapat melakukan feature scaling karena data processed tidak tersedia\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling dan Engineering menggunakan PyCaret\n",
    "print(\"=== FEATURE SCALING DAN ENGINEERING DENGAN PYCARET ===\")\n",
    "\n",
    "if df_processed is not None:\n",
    "    \n",
    "    # Analisis distribusi untuk scaling\n",
    "    print(f\"\\nAnalisis distribusi fitur:\")\n",
    "    for feature in features[:2]:  # Show 2 features saja\n",
    "        stats = df_processed[feature].describe()\n",
    "        print(f\"   â€¢ {feature.split('(')[0].strip()}: Mean={stats['mean']:.2f}, Std={stats['std']:.2f}\")\n",
    "    \n",
    "    # Setup PyCaret preprocessing\n",
    "    try:\n",
    "        from pycaret.classification import setup as classification_setup, get_config\n",
    "        \n",
    "        print(f\"\\nSetting up PyCaret preprocessing...\")\n",
    "        \n",
    "        # Setup PyCaret dengan feature scaling\n",
    "        clf_env = classification_setup(\n",
    "            data=df_processed[features + ['species']],\n",
    "            target='species',\n",
    "            session_id=42,\n",
    "            train_size=0.8,\n",
    "            normalize=True,\n",
    "            normalize_method='zscore',\n",
    "            feature_interaction=True,\n",
    "            feature_ratio=True,\n",
    "            polynomial_features=False,\n",
    "            silent=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Get processed data\n",
    "        X_train_pycaret = get_config('X_train')\n",
    "        X_test_pycaret = get_config('X_test') \n",
    "        y_train_pycaret = get_config('y_train')\n",
    "        y_test_pycaret = get_config('y_test')\n",
    "        \n",
    "        print(f\"\\nPyCaret preprocessing berhasil:\")\n",
    "        print(f\"   â€¢ Features awal: {len(features)}\")\n",
    "        print(f\"   â€¢ Features processed: {X_train_pycaret.shape[1]}\")\n",
    "        print(f\"   â€¢ Training samples: {X_train_pycaret.shape[0]}\")\n",
    "        print(f\"   â€¢ Test samples: {X_test_pycaret.shape[0]}\")\n",
    "        \n",
    "        # Buat final datasets\n",
    "        df_pycaret_processed = X_train_pycaret.copy()\n",
    "        df_pycaret_processed['species'] = y_train_pycaret\n",
    "        df_pycaret_test = X_test_pycaret.copy()\n",
    "        df_pycaret_test['species'] = y_test_pycaret\n",
    "        \n",
    "        scaling_methods = {\n",
    "            'pycaret': {\n",
    "                'data': df_pycaret_processed,\n",
    "                'test_data': df_pycaret_test,\n",
    "                'scaler': 'PyCaret Pipeline',\n",
    "                'description': 'PyCaret Z-score + Feature Engineering'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        recommended_method = 'pycaret'\n",
    "        all_features = list(X_train_pycaret.columns)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PyCaret error: {e}\")\n",
    "        print(\"Menggunakan manual scaling...\")\n",
    "        \n",
    "        # Manual scaling fallback\n",
    "        from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "        \n",
    "        scaling_methods = {}\n",
    "        \n",
    "        # StandardScaler\n",
    "        scaler_standard = StandardScaler()\n",
    "        df_standard = df_processed.copy()\n",
    "        df_standard[features] = scaler_standard.fit_transform(df_processed[features])\n",
    "        scaling_methods['standard'] = {\n",
    "            'data': df_standard,\n",
    "            'scaler': scaler_standard,\n",
    "            'description': 'StandardScaler (Z-score)'\n",
    "        }\n",
    "        \n",
    "        # RobustScaler\n",
    "        scaler_robust = RobustScaler()\n",
    "        df_robust = df_processed.copy()\n",
    "        df_robust[features] = scaler_robust.fit_transform(df_processed[features])\n",
    "        scaling_methods['robust'] = {\n",
    "            'data': df_robust,\n",
    "            'scaler': scaler_robust,\n",
    "            'description': 'RobustScaler (Robust to outliers)'\n",
    "        }\n",
    "        \n",
    "        # Manual feature engineering\n",
    "        for method_name, method_info in scaling_methods.items():\n",
    "            df_current = method_info['data']\n",
    "            \n",
    "            # Simple feature engineering\n",
    "            df_current['petal_area'] = df_current['petal length (cm)'] * df_current['petal width (cm)']\n",
    "            df_current['sepal_area'] = df_current['sepal length (cm)'] * df_current['sepal width (cm)']\n",
    "            df_current['petal_sepal_ratio'] = df_current['petal length (cm)'] / df_current['sepal length (cm)']\n",
    "            \n",
    "            scaling_methods[method_name]['data'] = df_current\n",
    "        \n",
    "        engineered_features = ['petal_area', 'sepal_area', 'petal_sepal_ratio']\n",
    "        all_features = features + engineered_features\n",
    "        \n",
    "        print(f\"\\nManual scaling selesai:\")\n",
    "        print(f\"   â€¢ Original features: {len(features)}\")\n",
    "        print(f\"   â€¢ Engineered features: {len(engineered_features)}\")\n",
    "        print(f\"   â€¢ Total features: {len(all_features)}\")\n",
    "        \n",
    "        recommended_method = 'robust' if df_processed['strong_consensus'].sum() > 0 else 'standard'\n",
    "    print(f\"\\nMETODE YANG DIPILIH: {recommended_method.upper()}\")\n",
    "    \n",
    "    # Set final dataset\n",
    "    if 'test_data' in scaling_methods[recommended_method]:\n",
    "        # PyCaret case\n",
    "        df_final = scaling_methods[recommended_method]['data']\n",
    "        df_test_final = scaling_methods[recommended_method]['test_data']\n",
    "        \n",
    "        print(f\"\\nPyCaret Dataset Ready:\")\n",
    "        print(f\"   â€¢ Training: {df_final.shape}\")\n",
    "        print(f\"   â€¢ Test: {df_test_final.shape}\")\n",
    "        print(f\"   â€¢ Features: {len(all_features)}\")\n",
    "        \n",
    "    else:\n",
    "        # Manual case\n",
    "        df_final = scaling_methods[recommended_method]['data'].copy()\n",
    "        \n",
    "        print(f\"\\nManual Dataset Ready:\")\n",
    "        print(f\"   â€¢ Total data: {df_final.shape}\")\n",
    "        print(f\"   â€¢ Features: {len(all_features)}\")\n",
    "        print(f\"   â€¢ Siap untuk train-test split\")\n",
    "    \n",
    "    print(f\"\\nâœ“ PREPROCESSING SELESAI\")\n",
    "    print(f\"   Dataset siap untuk modeling phase\")\n",
    "\n",
    "else:\n",
    "    print(\"Tidak dapat melakukan feature scaling karena data processed tidak tersedia\")\n",
    "    df_final = None\n",
    "    df_test_final = None\n",
    "    final_scaler = None\n",
    "    all_features = features if 'features' in locals() else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbffce",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split dan Persiapan Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b88b8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN-TEST SPLIT DAN PERSIAPAN FINAL DENGAN PYCARET ===\n",
      "Tidak dapat melakukan train-test split karena dataset final tidak tersedia\n"
     ]
    }
   ],
   "source": [
    "# Train-Test Split dan Persiapan Dataset untuk Modeling dengan PyCaret\n",
    "print(\"=== TRAIN-TEST SPLIT DAN PERSIAPAN FINAL DENGAN PYCARET ===\")\n",
    "\n",
    "if df_final is not None:\n",
    "    \n",
    "    # Check apakah sudah ada split dari PyCaret atau perlu manual split\n",
    "    if df_test_final is not None:\n",
    "        # PyCaret sudah melakukan split\n",
    "        print(f\"Menggunakan PyCaret train-test split:\")\n",
    "        print(f\"   â€¢ PyCaret telah melakukan stratified split otomatis\")\n",
    "        print(f\"   â€¢ Train set: {df_final.shape[0]} samples\")\n",
    "        print(f\"   â€¢ Test set: {df_test_final.shape[0]} samples\")\n",
    "        \n",
    "        # Extract features dan target dari PyCaret results\n",
    "        target_col = 'species'\n",
    "        feature_cols = [col for col in df_final.columns if col != target_col]\n",
    "        \n",
    "        X_train = df_final[feature_cols].copy()\n",
    "        y_train = df_final[target_col].copy()\n",
    "        X_test = df_test_final[feature_cols].copy()\n",
    "        y_test = df_test_final[target_col].copy()\n",
    "        \n",
    "        all_features = feature_cols\n",
    "        \n",
    "        print(f\"   â€¢ Features dari PyCaret: {len(all_features)}\")\n",
    "        \n",
    "    else:\n",
    "        # Manual split untuk non-PyCaret case\n",
    "        print(f\"Melakukan manual train-test split:\")\n",
    "        \n",
    "        # Persiapan features dan target\n",
    "        X = df_final[all_features].copy()\n",
    "        y = df_final['species'].copy()\n",
    "        \n",
    "        print(f\"Dataset Preparation:\")\n",
    "        print(f\"   â€¢ Total samples: {len(df_final)}\")\n",
    "        print(f\"   â€¢ Features: {len(all_features)}\")\n",
    "        print(f\"   â€¢ Target classes: {y.nunique()} (setosa=0, versicolor=1, virginica=2)\")\n",
    "        \n",
    "        # Train-test split dengan stratified sampling\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"   â€¢ Train set: {len(X_train)} samples\")\n",
    "        print(f\"   â€¢ Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Cek distribusi kelas\n",
    "    print(f\"\\nDistribusi Kelas:\")\n",
    "    species_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "    \n",
    "    # Train distribution\n",
    "    train_dist = y_train.value_counts().sort_index()\n",
    "    test_dist = y_test.value_counts().sort_index()\n",
    "    \n",
    "    print(f\"{'Class':<12} {'Train Count':<12} {'Train %':<10} {'Test Count':<11} {'Test %':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for class_idx in [0, 1, 2]:\n",
    "        train_count = train_dist.get(class_idx, 0)\n",
    "        test_count = test_dist.get(class_idx, 0)\n",
    "        train_pct = (train_count / len(y_train)) * 100 if len(y_train) > 0 else 0\n",
    "        test_pct = (test_count / len(y_test)) * 100 if len(y_test) > 0 else 0\n",
    "        \n",
    "        print(f\"{species_names[class_idx]:<12} {train_count:<12} {train_pct:<9.1f}% {test_count:<11} {test_pct:<7.1f}%\")\n",
    "    \n",
    "    # Informasi preprocessing untuk reproduksi\n",
    "    preprocessing_method = 'pycaret' if df_test_final is not None else 'manual'\n",
    "    \n",
    "    preprocessing_info = {\n",
    "        'outlier_treatment': 'consensus_based_removal',\n",
    "        'scaling_method': preprocessing_method,\n",
    "        'original_features': features,\n",
    "        'processed_features': all_features,\n",
    "        'total_features': len(all_features),\n",
    "        'scaler': final_scaler,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'random_state': 42,\n",
    "        'preprocessing_pipeline': preprocessing_method\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPreprocessing Information:\")\n",
    "    print(f\"   â€¢ Outlier treatment: {preprocessing_info['outlier_treatment']}\")\n",
    "    print(f\"   â€¢ Scaling method: {preprocessing_info['scaling_method']}\")\n",
    "    print(f\"   â€¢ Total features: {preprocessing_info['total_features']}\")\n",
    "    print(f\"   â€¢ Random state: {preprocessing_info['random_state']}\")\n",
    "    \n",
    "    # Tampilkan sample dari dataset final\n",
    "    print(f\"\\nSample Data Final (Train Set - First 5 rows):\")\n",
    "    if preprocessing_method == 'pycaret':\n",
    "        # Show beberapa features penting dari PyCaret\n",
    "        sample_features = list(X_train.columns)[:4]  # First 4 features dari PyCaret\n",
    "    else:\n",
    "        # Show original + engineered features\n",
    "        sample_features = ['sepal length (cm)', 'petal length (cm)', 'petal_area', 'total_area']\n",
    "    \n",
    "    print(X_train[sample_features].head())\n",
    "    \n",
    "    print(f\"\\nTarget Labels (Train Set - First 10):\")\n",
    "    print(y_train.head(10).tolist())\n",
    "    \n",
    "    # PyCaret Data Preparation untuk modeling\n",
    "    print(f\"\\nPERSIAPAN UNTUK PYCARET MODELING:\")\n",
    "    \n",
    "    if preprocessing_method == 'pycaret':\n",
    "        # Data sudah diproses oleh PyCaret\n",
    "        df_pycaret = df_final.copy()\n",
    "        df_pycaret_test = df_test_final.copy()\n",
    "        \n",
    "        print(f\"   â€¢ Data telah diproses oleh PyCaret preprocessing pipeline\")\n",
    "        print(f\"   â€¢ df_pycaret: {df_pycaret.shape} (training data)\")\n",
    "        print(f\"   â€¢ df_pycaret_test: {df_pycaret_test.shape} (test data)\")\n",
    "        print(f\"   â€¢ Target column: 'species'\")\n",
    "        print(f\"   â€¢ Siap langsung untuk modeling\")\n",
    "        \n",
    "    else:\n",
    "        # Gabungkan X dan y untuk PyCaret\n",
    "        df_pycaret = X_train.copy()\n",
    "        df_pycaret['species'] = y_train\n",
    "        \n",
    "        # Siapkan test set terpisah untuk evaluasi final\n",
    "        df_pycaret_test = X_test.copy()\n",
    "        df_pycaret_test['species'] = y_test\n",
    "        \n",
    "        print(f\"   â€¢ df_pycaret: {df_pycaret.shape} (untuk training & validation)\")\n",
    "        print(f\"   â€¢ df_pycaret_test: {df_pycaret_test.shape} (untuk final evaluation)\")\n",
    "        print(f\"   â€¢ Target column: 'species'\")\n",
    "        print(f\"   â€¢ Feature columns: {len(all_features)} features\")\n",
    "    \n",
    "    # Summary informasi untuk modeling\n",
    "    print(f\"\\nSUMMARY PREPROCESSING:\")\n",
    "    print(f\"   â€¢ Objective: Multi-class classification (3 classes)\")\n",
    "    print(f\"   â€¢ Data quality: High (outliers handled dengan PyCaret)\")\n",
    "    print(f\"   â€¢ Preprocessing: {preprocessing_method.upper()} pipeline\")\n",
    "    print(f\"   â€¢ Features: {len(all_features)} total features\")\n",
    "    print(f\"   â€¢ Class balance: Good (stratified split maintained)\")\n",
    "    print(f\"   â€¢ Ready for: PyCaret automated ML pipeline\")\n",
    "    \n",
    "    print(f\"\\nPREPROCESSING DENGAN PYCARET COMPLETED!\")\n",
    "    print(f\"Dataset siap untuk modeling dengan PyCaret\")\n",
    "    \n",
    "else:\n",
    "    print(\"Tidak dapat melakukan train-test split karena dataset final tidak tersedia\")\n",
    "    X_train = X_test = y_train = y_test = None\n",
    "    df_pycaret = df_pycaret_test = None\n",
    "    preprocessing_info = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fde19e",
   "metadata": {},
   "source": [
    "## 6. Summary dan Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7a8a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "        SUMMARY DATA PREPROCESSING\n",
      "==================================================\n",
      "\n",
      "YANG SUDAH DILAKUKAN:\n",
      "   âœ“ Load dataset Iris (150 data)\n",
      "   âœ“ Deteksi outlier dengan 3 model (ABOD, KNN, COF)\n",
      "   âœ“ Hapus outlier dan isi dengan imputation\n",
      "   âœ“ Scaling dan feature engineering\n",
      "   âœ“ Split data train-test\n",
      "\n",
      "LANGKAH SELANJUTNYA:\n",
      "   1. Modeling dengan PyCaret\n",
      "   2. Evaluasi performa model\n",
      "   3. Pilih model terbaik\n",
      "\n",
      "âš  Jalankan semua cell preprocessing dulu\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY PREPROCESSING SEDERHANA\n",
    "print(\"=\" * 50)\n",
    "print(\"        SUMMARY DATA PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nYANG SUDAH DILAKUKAN:\")\n",
    "print(\"   âœ“ Load dataset Iris (150 data)\")\n",
    "print(\"   âœ“ Deteksi outlier dengan 3 model (ABOD, KNN, COF)\")\n",
    "print(\"   âœ“ Hapus outlier dan isi dengan imputation\")\n",
    "print(\"   âœ“ Scaling dan feature engineering\")\n",
    "print(\"   âœ“ Split data train-test\")\n",
    "\n",
    "if 'preprocessing_info' in locals() and preprocessing_info is not None:\n",
    "    train_size = preprocessing_info.get('train_size', 'N/A')\n",
    "    test_size = preprocessing_info.get('test_size', 'N/A')\n",
    "    features_count = preprocessing_info.get('total_features', 'N/A')\n",
    "    \n",
    "    print(f\"\\nHASIL AKHIR:\")\n",
    "    print(f\"   â€¢ Data train: {train_size} samples\")\n",
    "    print(f\"   â€¢ Data test: {test_size} samples\") \n",
    "    print(f\"   â€¢ Total fitur: {features_count}\")\n",
    "    print(f\"   â€¢ Kelas: 3 (setosa, versicolor, virginica)\")\n",
    "\n",
    "print(f\"\\nLANGKAH SELANJUTNYA:\")\n",
    "print(f\"   1. Modeling dengan PyCaret\")\n",
    "print(f\"   2. Evaluasi performa model\")\n",
    "print(f\"   3. Pilih model terbaik\")\n",
    "\n",
    "if 'df_pycaret' in locals() and df_pycaret is not None:\n",
    "    print(f\"\\nâœ“ Data siap untuk modeling!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Jalankan semua cell preprocessing dulu\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
